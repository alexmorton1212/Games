{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools as iter\n",
    "import json\n",
    "from json import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read In Master Word Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.read_csv('letterase-my-word-list.txt', sep=\"\\t\")\n",
    "double_df = words_df[['Word','Start2','End2']].rename(columns = {\"Word\":\"words\", \"Start2\":\"start\", \"End2\":\"end\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNTIME: 2 MINUTES (OR LESS) UNDER CURRENT PARAMETERS\n",
    "\n",
    "seed = 12\n",
    "\n",
    "# DOUBLE LETTER RESULTS IN 240,000 SOLUTIONS\n",
    "\n",
    "p1 = 0.4\n",
    "p2 = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONDUCT FIRST MERGE AND SAMPLE BASED ON PROBABILITIES\n",
    "\n",
    "renamed_columns_1 = {\"words_x\":\"w1\", \"start_x\":\"start1\", \"end_x\":\"end1\", \"words_y\":\"w2\", \"start_y\":\"start2\", \"end_y\":\"end2\"}\n",
    "\n",
    "merge_df_1 = pd.merge(double_df, double_df, left_on = 'end', right_on = 'start').rename(columns = renamed_columns_1)\n",
    "counts_df_1 = merge_df_1['end1'].value_counts().reset_index().rename(columns={'index': 'value', 0: 'count'})\n",
    "counts_df_1['p'] = (1/len(counts_df_1)) / counts_df_1['end1']\n",
    "counts_merge_df_1 = pd.merge(merge_df_1, counts_df_1, left_on = 'end1', right_on = 'value', how='left')\n",
    "counts_merge_df_1 = counts_merge_df_1[['w1','w2','start2','end2','p']]\n",
    "first_df = counts_merge_df_1.sample(n=round(len(counts_merge_df_1)*p1), random_state=seed, weights='p')\n",
    "first_df = first_df[['w1','w2','end2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONDUCT SECOND MERGE AND SAMPLE BASED ON PROBABILITIES\n",
    "\n",
    "renamed_columns_2 = {\"words\":\"w3\", \"start\":\"start3\", \"end\":\"end3\"}\n",
    "\n",
    "merge_df_2 = pd.merge(first_df, double_df, left_on = 'end2', right_on = 'start').rename(columns = renamed_columns_2)\n",
    "counts_df_2 = merge_df_2['end2'].value_counts().reset_index().rename(columns={'index': 'value', 0: 'count'})\n",
    "counts_df_2['p'] = (1/len(counts_df_2)) / counts_df_2['end2']\n",
    "counts_merge_df_2 = pd.merge(merge_df_2, counts_df_2, left_on = 'end2', right_on = 'value', how='left')\n",
    "counts_merge_df_2 = counts_merge_df_2[['w1','w2','w3','start3','end3','p']]\n",
    "second_df = counts_merge_df_2.sample(n=round(len(counts_merge_df_2)*p2), random_state=seed, weights='p')\n",
    "second_df = second_df[['w1','w2','w3','end3']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONDUCT THIRD MERGE AND SAMPLE BASED ON PROBABILITIES\n",
    "\n",
    "renamed_columns_3 = {\"words\":\"w4\", \"start\":\"start4\", \"end\":\"end4\"}\n",
    "\n",
    "merge_df_3 = pd.merge(second_df, double_df, left_on = 'end3', right_on = 'start').rename(columns = renamed_columns_3)\n",
    "counts_df_3 = merge_df_3['end3'].value_counts().reset_index().rename(columns={'index': 'value', 0: 'count'})\n",
    "counts_df_3['p'] = (1/len(counts_df_3)) / counts_df_3['end3']\n",
    "counts_merge_df_3 = pd.merge(merge_df_3, counts_df_3, left_on = 'end3', right_on = 'value', how='left')\n",
    "counts_merge_df_3 = counts_merge_df_3[['w1','w2','w3','w4','start4','end4','p']]\n",
    "third_df = counts_merge_df_3.sample(n=round(len(counts_merge_df_3)*p2), random_state=seed, weights='p')\n",
    "third_df = third_df[['w1','w2','w3','w4','end4']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONDUCT FINAL MERGE AND SAMPLE BASED ON PROBABILITIES\n",
    "\n",
    "renamed_columns_4 = {\"words\":\"w5\", \"start\":\"start5\", \"end\":\"end5\"}\n",
    "\n",
    "merge_df_4 = pd.merge(third_df, double_df, left_on = 'end4', right_on = 'start').rename(columns = renamed_columns_4)\n",
    "counts_df_4 = merge_df_4['end4'].value_counts().reset_index().rename(columns={'index': 'value', 0: 'count'})\n",
    "counts_df_4['p'] = (1/len(counts_df_4)) / counts_df_4['end4']\n",
    "counts_merge_df_4 = pd.merge(merge_df_4, counts_df_4, left_on = 'end4', right_on = 'value', how='left')\n",
    "counts_merge_df_4 = counts_merge_df_4[['w1','w2','w3','w4','w5','start5','end5','p']]\n",
    "final_df = counts_merge_df_4.sample(n=round(len(counts_merge_df_4)*p2), random_state=seed, weights='p')\n",
    "final_df = final_df[['w1','w2','w3','w4','w5']]\n",
    "\n",
    "# REMOVE ROWS WITH DUPLICATES\n",
    "\n",
    "final_df = final_df[~final_df.apply(lambda x: x.duplicated().any(), axis=1)]\n",
    "\n",
    "# FORMAT FINAL DATAFRAME\n",
    "\n",
    "renamed_columns_final = {\"w1\":\"words_1\", \"w2\":\"words_2\", \"w3\":\"words_3\", \"w4\":\"words_4\", \"w5\":\"words_5\"}\n",
    "final_df = final_df.rename(columns = renamed_columns_final).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Solution List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words_1</th>\n",
       "      <th>words_2</th>\n",
       "      <th>words_3</th>\n",
       "      <th>words_4</th>\n",
       "      <th>words_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aroma</td>\n",
       "      <td>manga</td>\n",
       "      <td>gamut</td>\n",
       "      <td>utile</td>\n",
       "      <td>legal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crepe</td>\n",
       "      <td>perch</td>\n",
       "      <td>cheap</td>\n",
       "      <td>apnea</td>\n",
       "      <td>earth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>delve</td>\n",
       "      <td>vegan</td>\n",
       "      <td>annex</td>\n",
       "      <td>extra</td>\n",
       "      <td>ratty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dogma</td>\n",
       "      <td>mange</td>\n",
       "      <td>genre</td>\n",
       "      <td>rerun</td>\n",
       "      <td>undid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>outdo</td>\n",
       "      <td>dough</td>\n",
       "      <td>ghost</td>\n",
       "      <td>steep</td>\n",
       "      <td>epoch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109198</th>\n",
       "      <td>grasp</td>\n",
       "      <td>spasm</td>\n",
       "      <td>smash</td>\n",
       "      <td>shoal</td>\n",
       "      <td>along</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109199</th>\n",
       "      <td>leach</td>\n",
       "      <td>chafe</td>\n",
       "      <td>fetch</td>\n",
       "      <td>chore</td>\n",
       "      <td>reuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109200</th>\n",
       "      <td>holly</td>\n",
       "      <td>lymph</td>\n",
       "      <td>photo</td>\n",
       "      <td>touch</td>\n",
       "      <td>check</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109201</th>\n",
       "      <td>globe</td>\n",
       "      <td>bench</td>\n",
       "      <td>chasm</td>\n",
       "      <td>smith</td>\n",
       "      <td>third</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109202</th>\n",
       "      <td>heave</td>\n",
       "      <td>venom</td>\n",
       "      <td>ombre</td>\n",
       "      <td>rebel</td>\n",
       "      <td>elude</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109203 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       words_1 words_2 words_3 words_4 words_5\n",
       "0        aroma   manga   gamut   utile   legal\n",
       "1        crepe   perch   cheap   apnea   earth\n",
       "2        delve   vegan   annex   extra   ratty\n",
       "3        dogma   mange   genre   rerun   undid\n",
       "4        outdo   dough   ghost   steep   epoch\n",
       "...        ...     ...     ...     ...     ...\n",
       "109198   grasp   spasm   smash   shoal   along\n",
       "109199   leach   chafe   fetch   chore   reuse\n",
       "109200   holly   lymph   photo   touch   check\n",
       "109201   globe   bench   chasm   smith   third\n",
       "109202   heave   venom   ombre   rebel   elude\n",
       "\n",
       "[109203 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT SOLUTIONS\n",
    "\n",
    "result = final_df.to_json(orient=\"index\")\n",
    "output_data = 'double_data.json'\n",
    "\n",
    "with open(output_data, 'w') as f:\n",
    "    dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Word List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.read_csv(\"letterase-my-word-list.txt\", sep=\"\\t\")['Word'].tolist()\n",
    "words_upper = [s.upper() for s in words]\n",
    "json_string = json.dumps(words_upper)\n",
    "\n",
    "with open(\"word_list.json\", \"w\") as f:\n",
    "    json.dump(words_upper, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
